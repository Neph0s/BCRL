import json
import requests
import time
import os
import pandas as pd
import pdb
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from utils import get_response, save_result, save_result_txt, extract_json, ensure_question_format
import random 
from utils import set_cache_path

# é…ç½®æ–¹æ³•é€‰æ‹©
search_model = 'gemini_search'  # é€‰æ‹© 'gemini_search' æˆ– 'deer-flow'
question_model = 'claude-4-sonnet' #'gpt'
language = 'en'  # é€‰æ‹© 'zh' æˆ– 'en'
entity_files = ['my_entities_en.csv', 'wikidata_entities_with_popularity_en_0625.csv'] 
output_file = 'bc_questions_0627_en.json'
existing_files = ["results/bc_questions_0625_en.json"]  # å·²æœ‰çš„æ•°æ®æ–‡ä»¶
parallel = True
set_cache_path('.cache-bc_questions_0625_en.pkl') # '.cache-' + output_file.replace('.json', '.pkl'))

progress_count = 0
progress_lock = threading.Lock()
total_entities = 0
save_interval = 100  # æ¯100ä¸ªå®ä½“ä¿å­˜ä¸€æ¬¡

def to_my_entity_key(entity_info):
	if 'entity_info' in entity_info:
		return entity_info['entity_info']['label'] + '[WIKI:' +str(entity_info['entity_info']['id']) + ']'
	else:
		return  entity_info['label'] + '[WIKI:' +str(entity_info['id']) + ']'

def save_progress(results, filename=None):
	"""ç»Ÿä¸€çš„ä¿å­˜è¿›åº¦å‡½æ•°"""
	if filename is None:
		filename = output_file
	
	os.makedirs("results", exist_ok=True)
	with open(f'results/{filename}', 'w', encoding='utf-8') as f:
		json.dump(results, f, ensure_ascii=False, indent=2)

def process_entity(entity_info):
	"""å¤„ç†å•ä¸ªå®ä½“çš„å‡½æ•°ï¼Œç”¨äºå¹¶å‘æ‰§è¡Œ"""
	global progress_count, total_entities
	
	entity_name = entity_info['label']
	entity_description = entity_info.get('description', '')
	
	with progress_lock:
		progress_count += 1
		current = progress_count
	
	print(f"[{current}/{total_entities}] å¼€å§‹æŸ¥è¯¢å®ä½“: {entity_name}")
	#import pdb; pdb.set_trace()
	# ä¿å­˜å®Œæ•´çš„å®ä½“ä¿¡æ¯
	result = {
		'entity': entity_name,
		'entity_info': entity_info.to_dict() if hasattr(entity_info, 'to_dict') else entity_info
	} 
	
	from prompts import get_prompt

	messages = []

	# æœé›†å®ä½“ä¿¡æ¯ - ç¬¬ä¸€æ¬¡ä½¿ç”¨label + description
	search_prompt = get_prompt('search_prompt', language)
	entity_full = f"{entity_name}({entity_description})" if entity_description else entity_name
	prompt = search_prompt.replace('{entity}', entity_full, 1).replace('{entity}', entity_name)
	messages.append({'role': 'user', 'content': prompt})
	knowledge = get_response(model=search_model, messages=messages)
	result['search_response'] = knowledge
	messages.append({'role': 'assistant', 'content': knowledge})

	# äºŒæ¬¡æ‰©å±•
	search_second_prompt = get_prompt('search_second_prompt', language)
	messages.append({'role': 'user', 'content': search_second_prompt})
	knowledge2 = get_response(model=search_model, messages=messages)
	result['search_again_response'] = knowledge2
	messages.append({'role': 'assistant', 'content': knowledge2})
	
	# ç”Ÿæˆé—®é¢˜ - åç»­åªä½¿ç”¨label
	# question_generate_prompt = get_prompt('question_generate_prompt', language)
	# for N_I_LOW, N_I_HIGH in [(3, 4), (5, 6)]:
	# 	prompt = question_generate_prompt.replace('{entity}', entity_name).replace('{N_I_LOW}', str(N_I_LOW)).replace('{N_I_HIGH}', str(N_I_HIGH)).replace('{N_Q}', '3')

	# 	messages.append({'role': 'user', 'content': prompt})
	# 	response = get_response([extract_json, ensure_question_format], model=question_model, messages=messages)

	# 	if 'question_response' not in result:
	# 		result['question_response'] = []
		
	# 	result['question_response'].append(response)
		
	# 	messages.pop()

	# 	# ç¦»çº¿åˆ¤å®šç­”æ¡ˆå”¯ä¸€æ€§ TODO

	return result


def main():
	global total_entities
	
	# å…ˆè¯»å–å·²æœ‰ç»“æœæ–‡ä»¶
	existing_results = {}
	existing_entitiy_keys = set()
	
	for existing_file in existing_files:
		if os.path.exists(existing_file):
			print(f"è¯»å–å·²æœ‰ç»“æœæ–‡ä»¶: {existing_file}")
			with open(existing_file, 'r', encoding='utf-8') as f:
				existing_data = json.load(f)
				# è½¬æ¢æ—§æ ¼å¼keyä¸ºæ–°æ ¼å¼
				for key, value in existing_data.items():
					if 'WIKI:' in key:
						new_key = key
					else:
						new_key = to_my_entity_key(value)
					existing_results[new_key] = value
			print(f"å·²è¯»å– {len(existing_data)} ä¸ªå·²æœ‰å®ä½“")
		else:
			print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {existing_file}")
	
	existing_entitiy_keys = set(existing_results.keys())
	print(f"æ€»å…±å·²æœ‰ {len(existing_entitiy_keys)} ä¸ªå®ä½“")
	
	# è¯»å–å¤šä¸ªå®ä½“æ–‡ä»¶
	entities_data = []
	import pdb; pdb.set_trace()
	n_entities = 10100
	# æŒ‰ä¼˜å…ˆçº§é¡ºåºè¯»å–å„ä¸ªæ–‡ä»¶
	for i_f, entity_file in enumerate(entity_files):
		df = pd.read_csv(entity_file)
		print(f"æˆåŠŸè¯»å– {entity_file}ï¼Œå…± {len(df)} æ¡è®°å½•")
		
		# è¯»å–å®ä½“ä¿¡æ¯ï¼Œæ·»åŠ åˆ°åˆ—è¡¨ä¸­ï¼Œæ’é™¤å·²æœ‰å®ä½“
		_entities_data = []
		for _, row in df.iterrows():
			entity_dict = row.to_dict()
			# åªæ·»åŠ ä¸åœ¨å·²æœ‰å®ä½“ä¸­çš„æ–°å®ä½“
			if to_my_entity_key(entity_dict) not in existing_entitiy_keys:
				_entities_data.append(entity_dict)
		
		print(f"æ’é™¤å·²æœ‰å®ä½“åï¼Œå‰©ä½™ {len(_entities_data)} ä¸ªæ–°å®ä½“")

		# 0627:è¿‡æ»¤ï¼Œä»…ä¿ç•™popularity_score < 10000çš„å®ä½“	
		_entities_data = [entity_dict for entity_dict in _entities_data if entity_dict['popularity_score'] < 10000]
		print(f"æŒ‰entity_info['popularity_score'] < 10000è¿‡æ»¤åï¼Œå‰©ä½™ {len(_entities_data)} ä¸ªæ–°å®ä½“")

		assert i_f < 2
		if i_f == 1:
			n_sample = n_entities - len(existing_entitiy_keys) - len(entities_data)
			from utils import stable_shuffle
			entities_data.extend(stable_shuffle(_entities_data)[:n_sample])
		else:
			entities_data.extend(_entities_data)
	# å±•ç¤ºpopularityåˆ†å¸ƒ
	print(f"æ€»å…±è¯»å–äº† {len(entities_data)} ä¸ªå®ä½“")
	
	if 1:
		# æå–popularityä¿¡æ¯å¹¶å±•ç¤ºåˆ†å¸ƒ
		popularities = []
		for entity in entities_data:
			if 'popularity_score' in entity and entity['popularity_score'] is not None:
				score = float(entity['popularity_score'])
				import math 
				if math.isnan(score): score = 0
				
				popularities.append(score)
		
		if popularities:
			import numpy as np
			print(f"\n=== Popularityåˆ†å¸ƒç»Ÿè®¡ ===")
			print(f"æœ‰æ•ˆpopularityè®°å½•æ•°: {len(popularities)}")
			print(f"æœ€å°å€¼: {np.min(popularities):.4f}")
			print(f"æœ€å¤§å€¼: {np.max(popularities):.4f}")
			print(f"å¹³å‡å€¼: {np.mean(popularities):.4f}")
			print(f"ä¸­ä½æ•°: {np.median(popularities):.4f}")
			print(f"æ ‡å‡†å·®: {np.std(popularities):.4f}")
			
			# æ˜¾ç¤ºåˆ†ä½æ•°
			percentiles = [10, 25, 50, 75, 90, 95, 99]
			print(f"\nåˆ†ä½æ•°åˆ†å¸ƒ:")
			for p in percentiles:
				print(f"  {p}%: {np.percentile(popularities, p):.4f}")
			
			# æ˜¾ç¤ºåˆ†å¸ƒåŒºé—´ç»Ÿè®¡
			bins = [0, 0.001, 0.01, 0.1, 1.0, float('inf')]
			bin_labels = ['<0.001', '0.001-0.01', '0.01-0.1', '0.1-1.0', '>=1.0']
			print(f"\nåˆ†å¸ƒåŒºé—´ç»Ÿè®¡:")
			for i in range(len(bins)-1):
				count = sum(1 for p in popularities if bins[i] <= p < bins[i+1])
				percentage = count / len(popularities) * 100
				print(f"  {bin_labels[i]}: {count} ({percentage:.1f}%)")
		else:
			print("æœªæ‰¾åˆ°æœ‰æ•ˆçš„popularityæ•°æ®")
	
	
	total_entities = len(entities_data)

	# æ ¹æ®æ–¹æ³•è°ƒæ•´å¹¶å‘æ•°
	
	# åˆå§‹åŒ–ç»“æœå­—å…¸ï¼ŒåŒ…å«å·²æœ‰ç»“æœå’Œæ–°å®ä½“
	results = existing_results.copy()  # å…ˆå¤åˆ¶å·²æœ‰ç»“æœ

	if parallel:
		max_workers = 15
		completed_count = 0
		
		with ThreadPoolExecutor(max_workers=max_workers) as executor:
			# æäº¤æ‰€æœ‰ä»»åŠ¡
			future_to_entity = {executor.submit(process_entity, entity_info): entity_info['label'] for entity_info in entities_data}
			
			# å¤„ç†å®Œæˆçš„ä»»åŠ¡
			for future in as_completed(future_to_entity):
				entity_name = future_to_entity[future]
				result = future.result()
				results[to_my_entity_key(result)] = result
				completed_count += 1
				
				# æ¯100ä¸ªå®ä½“ä¿å­˜ä¸€æ¬¡
				if completed_count % save_interval == 0:
					print(f"ğŸ’¾ å·²å®Œæˆ {completed_count} ä¸ªå®ä½“ï¼Œä¿å­˜ä¸­é—´ç»“æœ...")
					save_progress(results)
					print(f"ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: results/{output_file}")

	else:
		for i, entity_info in enumerate(entities_data, 1):
			result = process_entity(entity_info)
			results[to_my_entity_key(result)] = result
			
			# æ¯100ä¸ªå®ä½“ä¿å­˜ä¸€æ¬¡
			if i % save_interval == 0:
				print(f"ğŸ’¾ å·²å®Œæˆ {i} ä¸ªå®ä½“ï¼Œä¿å­˜ä¸­é—´ç»“æœ...")
				save_progress(results)
				print(f"ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: results/{output_file}")

	# ç»Ÿè®¡ç»“æœï¼šå·²æœ‰æ•°æ® + æ–°å®Œæˆçš„æ•°æ®
	total_completed = len([result for result in results.values() if result is not None])
	new_completed = len([result for entity_info in entities_data for result in [results[to_my_entity_key(entity_info)]] if result is not None])
	new_failed = len(entities_data) - new_completed
	
	print(f"ğŸ“Š ç»Ÿè®¡ç»“æœ:")
	print(f"  å·²æœ‰å®ä½“: {len(existing_entitiy_keys)}")
	print(f"  æ–°å¤„ç†å®ä½“: {len(entities_data)}")
	print(f"  æ–°æˆåŠŸ: {new_completed}, æ–°å¤±è´¥: {new_failed}")
	print(f"  æ€»è®¡æˆåŠŸ: {total_completed}, æ€»è®¡å®ä½“: {len(results)}")

	# ä¿å­˜æ±‡æ€»ç»“æœ
	save_progress(results, output_file)

	# ä¿å­˜TXTæ ¼å¼çš„ç®€åŒ–ç»“æœ
	save_result_txt(f'results/{output_file}_simple.txt', results)

	print(f"ğŸ“ ç»“æœä¿å­˜åœ¨ results/ ç›®å½•ä¸‹ï¼ˆä½¿ç”¨{question_model}æ–¹æ³•ï¼‰")
	print(f"ğŸ“„ JSONæ ¼å¼: results/{output_file}.json")
	print(f"ğŸ“„ TXTæ ¼å¼: results/{output_file}_simple.txt")

def print_questions():
	with open('results/claude-4-sonnet.json', 'r', encoding='utf-8') as f:
		results = json.load(f)

	simple_results = {entity: results[entity.split('[WIKI:')[0]]['question_response'] for entity in results}
	# for entity, result in results.items():
	#     print(entity)
	#     for question in result['question_response']:
	#         print(json.dumps(question, indent=4, ensure_ascii=False))

	with open('results/bc_questions_0625_zh.json', 'w', encoding='utf-8') as f:
		json.dump(simple_results, f, ensure_ascii=False, indent=4)

if __name__ == "__main__":
	main()
	#print_questions()